{"pages":[{"url":"http://tdeboissiere.github.io/pages/about.html","text":"Coming soon","tags":"pages","title":"About"},{"url":"http://tdeboissiere.github.io/pages/project.html","text":"Coming soon","tags":"pages","title":"Project"},{"url":"http://tdeboissiere.github.io/celeba-dataset-a-parallel-download-from-dropbox.html","text":"Introduction I recently got interested in face recognition with deep learning. I eventually chanced upon the CelebA dataset . It is freely available for academic purposes and has facial attributes annotations. However, it is a bit of a pain to download. It is split into 14 independent .zip on Dropbox. And you can't download all these files at the same time (probably because of server restrictions). So I decided to write a script to do the download for me. And for the sake of it, demonstrate how simple it is to parallelize processes with the python map function and the multiprocessing module. Install dependencies You should make sure to have urllib2 and bs4 installed. Strategy breakdown Use urllib2 and bs4 to parse the target url and look for the download links. Use subprocess to call the wget module (probably possible to do it with some python functions as well but I did not bother to search). Use the map function for easy multiprocessing. The code This code has been tailored to the dropbox page of the CelebA dataset . It should be simple enough to adapt to other purposes. Simply call: 1 python <name_of_the_file_you_put_the_code_in> For completeness sake, here are the running times: Download in parallel with this code : 515 sec for 2 .zip folders Download in serial : 540 sec for 2 .zip folders 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 from bs4 import BeautifulSoup import urllib2 import subprocess from multiprocessing.dummy import Pool as ThreadPool import time def download_from_dropbox ( url ): # Get the content of the url page resp = urllib2 . urlopen ( url ) # Parse it with beautifulsoup soup = BeautifulSoup ( resp , \"lxml\" , from_encoding = resp . info () . getparam ( 'charset' )) list_zip = [] # Look for the links in the soup for link in soup . find_all ( 'a' , href = True ): try : # Exploring the source code of said page shows # that the links I'm interested in have these properties if link [ \"class\" ] == [ \"file-link\" ]: list_zip . append ( link [ \"href\" ]) except KeyError : pass # Strip the \"?dl=0\" at the end of each link list_zip = [ f . split ( \"?dl=0\" )[ 0 ] for f in list_zip ] # Function we'll map to the url so that the calls # are in parallel def call_wget ( file_name ): subprocess . call ( 'wget ' + file_name , shell = True ) pool = ThreadPool ( 4 ) # Sets the pool size to 4 # Open the urls in their own threads # and return the results pool . map ( call_wget , list_zip ) # close the pool and wait for the work to finish pool . close () pool . join () if __name__ == '__main__' : links = [ \"https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AAAq9krDJxUMh1m0hbbxdnl4a/Img/img_celeba.7z?dl=0\" ] start = time . time () for url in links : download_from_dropbox ( url ) print time . time () - start","tags":"Deep Learning","title":"CelebA dataset: a parallel download from dropbox"},{"url":"http://tdeboissiere.github.io/opencv-and-scikit-image-for-image-inpainting.html","text":"Motivation I have been looking into the following problem: When you have images with quite different scales or resolution, it is not clear to me how well a convnet trained on low resolution images works on high resolution images or vice versa. Let's take a hypothetical example: You train a convnet on relatively large images which contain faces. You also have smaller images which are centered around faces. If you resize the smaller image to the scale of the larger one, you significantly distort the face and the face still occupies the whole image (unlike the images it was trained on). This may mean poor performance. To make the smaller image more similar to the large images, I have used the following strategy: Do not rescale the face. Instead, add borders to the smaller images to match the larger images' size. Use inpainting to fill the borders (it won't look natural but it's supposedly better than pure black). On to a simple example ! For completeness, we'll also compare the speed of OpenCV/scikit-image. Special dependencies openCV : 1 conda install -c menpo opencv3=3.1.0 scikit-image 1 conda install scikit-image The code We are going to inpaint the following image from the CelebA dataset Along the way, we'll also test the speed of OpenCV and scikit-image Simply call: 1 python <name_of_the_file_you_put_the_code_in> You should get a result looking like this: On this task, OpenCV was overwhelmingly faster ! On my machine, I got: Time inpainting a single image OpenCV: 0.0308220386505 Time inpainting a single image skimage: 85.2677919865 Code below (download the celebA image and name it 000001.jpg ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 import numpy as np import cv2 import matplotlib.pylab as plt import time from skimage.restoration import inpaint from skimage import io if __name__ == '__main__' : test_img = './000001.jpg' # Interactive plotting mode plt . ion () # Load image img = cv2 . imread ( test_img ) # Test load speed start = time . time () for i in range ( 32 ): img = cv2 . imread ( test_img ) print \"Time loading 32x times OpenCV:\" , time . time () - start # Create a bigger array in which we'll put the umage and then inpaint arr = np . zeros (( img . shape [ 0 ] + 50 , img . shape [ 1 ] + 50 , 3 )) arr [: img . shape [ 0 ], : img . shape [ 1 ], :] = img arr = arr . astype ( np . uint8 ) # Get the corresponding mask: # 1 where we need to inpaint, 0 elsewhere mask = np . zeros ( arr . shape [: 2 ]) . astype ( np . uint8 ) mask [ img . shape [ 0 ]:, :] = 1 mask [:, img . shape [ 1 ]:] = 1 # Time inpainting OpenCV start = time . time () dst = cv2 . inpaint ( arr , mask , 3 , cv2 . INPAINT_TELEA ) print \"Time inpainting OpenCV:\" , time . time () - start # Swap color channels (bgr to rgb) b , g , r = cv2 . split ( dst ) # get b,g,r img_inpaint = cv2 . merge ([ r , g , b ]) # switch it to rgb plt . imshow ( img_inpaint ) plt . show () raw_input () # Test load speed start = time . time () for i in range ( 32 ): img = io . imread ( './000001.jpg' ) print \"Time loading 32x times skimage:\" , time . time () - start # Create a bigger array in which we'll put the umage and then inpaint arr = np . zeros (( img . shape [ 0 ] + 50 , img . shape [ 1 ] + 50 , 3 )) arr [: img . shape [ 0 ], : img . shape [ 1 ], :] = img arr = arr . astype ( np . uint8 ) start = time . time () image_result = inpaint . inpaint_biharmonic ( arr , mask , multichannel = True ) print \"Time inpainting skimage:\" , time . time () - start plt . imshow ( image_result ) plt . show () raw_input ()","tags":"Deep Learning","title":"OpenCV and scikit-image for image inpainting"},{"url":"http://tdeboissiere.github.io/deep-learning-setup.html","text":"hese instructions are for Linux (Ubuntu) based but should be adaptable to other distros without too much work First make sure Ubuntu is up to date 1 2 3 4 sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential sudo apt-get autoremove Install gfortran (to compile OpenBlas) 1 sudo apt-get install gfortran Configure OpenBlas for fast linear algebra operations 1 2 3 4 5 cd git git clone https://github.com/xianyi/OpenBLAS cd OpenBLAS make FC=gfortran sudo make PREFIX=/usr/local install The tricky bit: getting your GPU up and running First verify your GPU is nVidia compatible 1 lspci | grep -i nvidia You should see something like 1 03 : 00.0 3 D controller : NVIDIA Corporation GM108M [ GeForce 830 M ] ( rev ff ) In my case, a GeForce 830M which is CUDA compatible: perfect ! Next we should make sure the drivers are up to date: 1 2 3 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-352 This is the official repository which should be safe to add to your PPA list. There are newer drivers but this one is the recommended one. Restart your system afterwards. Next step: configuring CUDA Install cuda Go to 1 https://developer.nvidia.com/cuda-downloads Select distribution and follow the instructions in the guide to set up CUDA. The guide is long because it deals with multiple Unix distributions. The core instructions are actually pretty limited. Points you should pay attention to: For some Unix distributions it is crucial to disable the nouveau drivers which may conflict with the nVidia ones. Make sure to add CUDA to your .bashrc or .zshrc The instructions for the last point are in the guide but let's paste them here for completeness: 1 2 3 echo 'export PATH=/usr/local/cuda/bin: $ PATH ' >> ~/.bashrc echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64: $ LD_LIBRARY_PATH ' >> ~/.bashrc source ~/.bashrc The guide also gives you the opportunity to test your CUDA installation by compiling some examples. Compile them with: 1 2 3 /usr/local/cuda/bin/cuda-install-samples-7.5.sh ~/cuda-samples cd ~/cuda-samples/NVIDIA*Samples make -j $(($(nproc) + 1)) The $(($(nproc) + 1)) statement uses all the available cores on your machines to compile faster. Next step is to install CuDNN. Install CUDNN Go to 1 https://developer.nvidia.com/cudnn Register, download and extract files Currently (Apr 2016), theano is not up to date with the latest cuda (cudnn v5) so for now, use cudnn v4. A <extractionpath>/cuda is created Copy the contents of the /cuda folder to your cuda installation repository On Ubuntum this should be : 1 2 3 cd <extractionpath>cuda sudo cp lib64/* /usr/local/cuda/lib64/ sudo cp include/cudnn.h /usr/local/cuda/include/ If everything's working fine, you can move on to the next step: configuring python for scientific computing. Configure python Download anaconda The Anaconda distribution has most libraries of interest and can super easily be set up on Linux, Windows and Mac: Go to 1 https://www.continuum.io/downloads Download the binaries and run 1 bash Anaconda-x.y.z-Linux-x86_64.sh (Linux) Install pip 1 conda install pip Install the optional pydot 1 pip install pydot You may have to change your version of pyparser if you get an error message. Now let's move on to deep learning libraries. We'll start with theano. Theano installation Install bleeding edge theano 1 pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git Configure theano with a .theanorc file In your home directory, create a .theanorc file in which you'll put the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [global] device = gpu floatX = float32 [blas] ldflags = -L/usr/local/lib -lopenblas [nvcc] fastmath = True [cuda] root = /usr/lib/nvidia-cuda-toolkit [dnn.conv] algo_bwd_filter = deterministic algo_bwd_data = deterministic The last flag may slow down the computations but allows deterministic results. To get faster convolutions, you should set it to: 1 2 3 [dnn.conv] algo_fwd = time_once algo_bwd_data = time_once Check theano sees cudnn 1 2 3 4 5 6 7 >>> from theano.sandbox.cuda.dnn import * Using gpu device 0 : Quadro K620 ( CNMeM is disabled , CuDNN 4007 ) >>> print dnn_available () True >>> print dnn_available . msg None >>> Theano test script Run this script to make sure theano can use the GPU/CPU normally ```python from theano import function, config, shared, sandbox import theano.tensor as T import numpy import time vlen = 10 * 30 * 768 # 10 x #cores x # threads per core iters = 1000 rng = numpy.random.RandomState(22) x = shared(numpy.asarray(rng.rand(vlen), config.floatX)) f = function([], T.exp(x)) print f.maker.fgraph.toposort() t0 = time.time() for i in xrange(iters): r = f() t1 = time.time() print 'Looping %d times took' % iters, t1 - t0, 'seconds' print 'Result is', r if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]): print 'Used the cpu' else: print 'Used the gpu' You can enable CNMeM with 1 2 [lib] cnmem = 0.01 in the .theanorc file The value represents the start size (either in MB or the fraction of total GPU memory) of the memory pool. If more memory is needed, Theano will try to obtain more, but this can cause memory fragmentation. Check speed of theano/OpenBlas 1 python `python -c \"import os, theano; print os.path.dirname(theano.__file__)\"` / misc / check_blas . py This will run some tests ans you can compare the speed of your setup with different reference setups Run some tests (optional and very slow...) 1 2 3 NumPy ( ~ 30 s ): python - c \"import numpy; numpy.test()\" SciPy ( ~ 1 m ): python - c \"import scipy; scipy.test()\" Theano ( ~ 30 m ): python - c \"import theano; theano.test()\" In case of an error message dealing with theano cache: 1 rm -rf $ HOME /.theano Now let's setup Caffe, which is somewhat more involved Caffe installation General dependencies 1 2 3 4 sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev sudo apt-get install libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev If an error is thrown with the boost library do : 1 2 3 4 5 6 sudo apt-get --purge remove libboost-all-dev libboost-dev libboost-doc sudo apt-get install -f sudo dpkg --configure -a sudo apt-get clean sudo apt-get update sudo apt-get install libboost1.54-dev Then relaunch the commands to install general dependencies Get caffe 1 2 3 4 5 git clone https://github.com/BVLC/caffe.git cp Makefile.config.example Makefile.config # Adjust Makefile.config # (See example at the end of this post) You should especially pay attention to the anaconda part, the opencv part, the use CUDNN part. If you have openblas installed, replace 1 2 Blas := atlas to BLAS := open Then: 1 2 3 4 make all -j $(($(nproc) + 1)) make pycaffe -j $(($(nproc) + 1)) make test -j $(($(nproc) + 1)) make runtest -j $(($(nproc) + 1)) More often than not, you may run into errors during comilation. Here are a few gotchas I've dealt with: Conflict with HDF5 Add anaconda2 to the lib path to link the proper hdf5 library. Also add the x86_64-linux-gnu path. In your .bashrc : 1 2 3 export LD_LIBRARY_PATH= $ LD_LIBRARY_PATH :/home/tmain/anaconda2/lib export LD_LIBRARY_PATH=/lib/x86_64-linux-gnu: $ LD_LIBRARY_PATH export PYTHONPATH=~/caffe/python: $ PYTHONPATH Getting pycaffe to work : In the folder caffe/python install all the requirements 1 for req in $( cat requirements.txt ) ; do pip install $ req ; done Then export python path : In your .basrhc file : 1 export PYTHONPATH=<caffe-home>/python: $ PYTHONPATH My Makefile.config for Caffe 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 ## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome! # cuDNN acceleration switch (uncomment to build with cuDNN). USE_CUDNN := 1 # CPU-only switch (uncomment to build without GPU support). # CPU_ONLY := 1 # uncomment to disable IO dependencies and corresponding data layers # USE_OPENCV := 0 # USE_LEVELDB := 0 # USE_LMDB := 0 # uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary) # You should not set this flag if you will be reading LMDBs with any # possibility of simultaneous read and write # ALLOW_LMDB_NOLOCK := 1 # Uncomment if you're using OpenCV 3 OPENCV_VERSION := 3 # To customize your choice of compiler, uncomment and set the following. # N.B. the default for Linux is g++ and the default for OSX is clang++ # CUSTOM_CXX := g++ # CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda # On Ubuntu 14.04, if cuda tools are installed via # \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead: # CUDA_DIR := /usr # CUDA architecture setting: going with all of them. # For CUDA < 6.0, comment the *_50 lines for compatibility. CUDA_ARCH := -gencode arch = compute_20,code = sm_20 \\ -gencode arch = compute_20,code = sm_21 \\ -gencode arch = compute_30,code = sm_30 \\ -gencode arch = compute_35,code = sm_35 \\ -gencode arch = compute_50,code = sm_50 \\ -gencode arch = compute_50,code = compute_50 # BLAS choice: # atlas for ATLAS (default) # mkl for MKL # open for OpenBlas BLAS := open # Custom (MKL/ATLAS/OpenBLAS) include and lib directories. # Leave commented to accept the defaults for your choice of BLAS # (which should work)! # BLAS_INCLUDE := /path/to/your/blas # BLAS_LIB := /path/to/your/blas # Homebrew puts openblas in a directory that is not on the standard search path # BLAS_INCLUDE := $(shell brew --prefix openblas)/include # BLAS_LIB := $(shell brew --prefix openblas)/lib # This is required only if you will compile the matlab interface. # MATLAB directory should contain the mex binary in /bin. # MATLAB_DIR := /usr/local # MATLAB_DIR := /Applications/MATLAB_R2012b.app # NOTE: this is required only if you will compile the python interface. # We need to be able to find Python.h and numpy/arrayobject.h. # PYTHON_INCLUDE := /usr/include/python2.7 \\ # /usr/lib/python2.7/dist-packages/numpy/core/include # Anaconda Python distribution is quite popular. Include path: # Verify anaconda location, sometimes it's in root. ANACONDA_HOME := $( HOME ) /anaconda2 PYTHON_INCLUDE := $( ANACONDA_HOME ) /include \\ $( ANACONDA_HOME ) /include/python2.7 \\ $( ANACONDA_HOME ) /lib/python2.7/site-packages/numpy/core/include \\ # Uncomment to use Python 3 (default is Python 2) # PYTHON_LIBRARIES := boost_python3 python3.5m # PYTHON_INCLUDE := /usr/include/python3.5m \\ # /usr/lib/python3.5/dist-packages/numpy/core/include # We need to be able to find libpythonX.X.so or .dylib. # PYTHON_LIB := /usr/lib PYTHON_LIB := /home/tmain/anaconda2/lib # Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1 # Whatever else you find you need goes here. INCLUDE_DIRS := $( PYTHON_INCLUDE ) /usr/local/include LIBRARY_DIRS := $( PYTHON_LIB ) /usr/local/lib /usr/lib # If Homebrew is installed at a non standard location # (for example your home directory) # and you use it for general dependencies # INCLUDE_DIRS += $(shell brew --prefix)/include # LIBRARY_DIRS += $(shell brew --prefix)/lib # Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries # are normally installed in one of the above $LIBRARY_DIRS.) # USE_PKG_CONFIG := 1 BUILD_DIR := build DISTRIBUTE_DIR := distribute # Uncomment for debugging. # Does not work on OSX due to https://github.com/BVLC/caffe/issues/171 # DEBUG := 1 # The ID of the GPU that 'make runtest' will use to run unit tests. TEST_GPUID := 0 # enable pretty build (comment to see full commands) Q ?= @","tags":"Deep Learning","title":"Deep Learning Setup"},{"url":"http://tdeboissiere.github.io/h5py-vs-npz.html","text":"Data engineering for computer vision Lately, I've been thinking hard about the best way to organize my data before feeding it to a machine learning classifier or regressor. I have a few guiding principles in mind: Keep the number of data files to a minimum Ease common operations (train test split, class selection) Make sure loading the data is not a bottleneck I first started with the numpy savez function which allows you to save the data in an ordered way. This would typically go as 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np # Assume you have a train and test array and labels: arr_train , label_train arr_test , label_test # Define a dict d = { \"arr_train\" : arr_train , \"arr_test\" : arr_test , \"label_train\" : label_train , \"label_test\" : label_test } np . savez ( \"data.npz\" , ** d ) Which works just fine. However, two limitations quickly became apparent: When you need to store a lot of metadata it quickly becomes a pain to organize. Loading the data from the disk is actually quite slow (see snippet below). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np import time arr = np . random . randint ( 0 , 1000 ,( 25000 , 5000 )) np . savez ( \"arr.npz\" , ** { \"arr\" : arr }) np . save ( \"arr.npy\" , arr ) ltime = [] for i in range ( 20 ): start = time . time () arr = np . load ( \"arr.npy\" )[:, :] ltime . append ( time . time () - start ) print \"npy time:\" , np . mean ( ltime ) ltime = [] for i in range ( 20 ): start = time . time () arr = np . load ( \"arr.npz\" )[ \"arr\" ][:, :] ltime . append ( time . time () - start ) print \"npz time:\" , np . mean ( ltime ) which gave me the following times: 1 2 npz time: 1.47687283754 npy time: 0.483348703384 That's quite the difference ! Clearly, another approach is needed. So far, I have settled with the excellent and simple h5py module which stores the data in HDF5 format while being very transparent to numpy. Here's how it goes: 1 2 3 4 5 6 7 8 import h5py import numpy as np import time arr = np . random . randint ( 0 , 1000 ,( 25000 , 5000 )) with h5py . File ( \"arr.h5\" , \"w\" ) as hf : hf . create_dataset ( \"arr\" , data = arr ) And that's it ! You can also easily add metadata to each of your datasets: 1 2 3 4 5 6 7 8 9 import h5py import numpy as np import time arr = np . random . randint ( 0 , 1000 ,( 25000 , 5000 )) with h5py . File ( \"arr.h5\" , \"w\" ) as hf : dset = hf . create_dataset ( \"arr\" , data = arr ) dset . attrs [ 'author' ] = \"pony\" My only gripe with the module was an ill-fated attempt at writing a file in parallel from several sources: you need to rebuild h5py to support parallelism (my anaconda distribution did not support it by default) and this takes you to a world of pain with conflicts between anaconda's own HDF5 library and the new parallel one you build. The only workaround I found involved reinstalling h5py outside of anaconda but messed with my MPI setup. Anyway, let's test the speed of this new design: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import h5py import numpy as np import time arr = np . random . randint ( 0 , 1000 ,( 25000 , 5000 )) with h5py . File ( \"arr.h5\" , \"w\" ) as hf : dset = hf . create_dataset ( \"arr\" , data = arr ) dset . attrs [ 'author' ] = \"pony\" ltime = [] for i in range ( 20 ): start = time . time () with h5py . File ( \"arr.h5\" , \"r\" ) as hf : arr = hf [ \"arr\" ][:, :] ltime . append ( time . time () - start ) print \"hdf5 time:\" , np . mean ( ltime ) This gave me: 1 hdf5 time: 0.386118304729 Which is even faster than the .npy version ! Later on, I'll try to give more details on my data pipeline.","tags":"Python","title":"h5py vs npz"},{"url":"http://tdeboissiere.github.io/aws-part-3-installing-python-and-custom-ami.html","text":"1. Launch an Amazon instance Launch an Amazon instance corresponding to your needs. For more information, check AWS Part 1 . 2. Install python software Once the instance is running, SSH to it as seen in AWS Part 1 . Download Anaconda with wget : wget https://3230d63b5fc54e62148e-c95ac804525aac4b6dba79b00b39d1d3.ssl.cf1.rackcdn.com/Anaconda2-2.4.1-Linux-x86_64.sh Install Anaconda : bash Anaconda2-2.4.1-Linux-x86_64.sh In case you need permission, just append sudo to each command Accept the conditions and the installation should start. When Anaconda asks whether you want to prepend its path to your $PATH variable, type yes. Reload your .bashrc source .bashrc If everything worked, you should see the following messages when calling python 1 2 3 4 5 Python 2.7.11 |Anaconda 2.4.1 (64-bit)| (default, Dec 6 2015, 18:08:32) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. Anaconda is brought to you by Continuum Analytics. Please check out: http://continuum.io/thanks and https://anaconda.org You can now import classic modules such as numpy, pandas etc. 3. Create an AMI for this instance In the AWS console, select the instance you just launched Actions => Image => Create Image . Give a name and a description to your instance, then Create Image . By default, Amazon will reboot your instance. You can choose not too by ticking the No Reboot window. The system is going down for reboot NOW! Control-Alt-Delete pressed Connection to ec2-52-62-15-17.ap-southeast-2.compute.amazonaws.com closed by remote host. Connection to ec2-52-62-15-17.ap-southeast-2.compute.amazonaws.com closed. By checking on the AMI pane in the console, you should see your image being created. You can now launch an instance from your new AMI. It contains all the packages that we installed in step 2.","tags":"AWS","title":"AWS Part 3 : Installing python and custom AMI"},{"url":"http://tdeboissiere.github.io/aws-part-2-ec2-instances.html","text":"1. Launch an instance Connect to the Amazon EC2 console Go to the EC2 dashboard , click on Launch instance . Then select the Ubuntu Server AMI. Stick to the t2.micro instance (free tier eligible). Go to Configure Security Group and select the group we configured in AWS Part 1 . We are ready to Launch Select the key pair we created in AWS Part 1 when prompted. A new key pair can also be created. Click Launch Instances N.B. Don't select the Proceed without a key pair option. If you launch your instance without a key pair, then you can't connect to it. N.B. It can take some time for the instance to launch. Review its status in Status Checks column. 2. Connect to your instance with an SSH Verify an SSH client is installed on your computer Install Amazon CLI tools. Download the tools from this link 1 wget http://s3.amazonaws.com/ec2-downloads/ec2-api-tools.zip And unzip in a suitable directory 1 2 sudo mkdir /usr/local/ec2 sudo unzip ec2-api-tools.zip -d /usr/local/ec2 Install and configure JAVA The Amazon EC2 CLI tools require Java. You can check Java is installed by running 1 which java which should yield something like : 1 /usr/bin/java If that is not the case, install java as indicated here . We now need to find the Java home directory. The which command we executed earlier returns Java's location in the $PATH environment variable but most of the time, it's a symbolic link. You can check this by running : 1 file $(which java) which in my case returns : 1 /usr/bin/java: symbolic link to `/etc/alternatives/java' by iterating the file command, you can find the true java home directory : 1 2 file /etc/alternatives/java > /etc/alternatives/java: symbolic link to `/usr/lib/jvm/java-8-oracle/jre/bin/java' the last location is the actual binary, which you can check by running : 1 2 file /usr/lib/jvm/java-8-oracle/jre/bin/java > /usr/lib/jvm/java-8-oracle/jre/bin/java: ELF 64-bit LSB executable In this example, the java home directory is : 1 /usr/lib/jvm/java-8-oracle/jre/ We are now going to set the JAVA_HOME variable to the home directory we identified : 1 export JAVA_HOME=\"/usr/lib/jvm/java-8-oracle/jre/\" To check this has been set correctly, use : 1 $ JAVA_HOME /bin/java -version which should get you : 1 2 3 java version \"1.8.0_66\" Java(TM) SE Runtime Environment (build 1.8.0_66-b17) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode) Add this variable definition to your .bashrc so that JAVA_HOME is defined whenever you spawn a new shell. Set the CLI Tools location The Amazon EC2 CLI tools read the EC2_HOME environment variable to locate supporting libraries. Before using these tools, set EC2_HOME to the directory path where you unzipped them. In your .bashrc, write : 1 export EC2_HOME=\"/usr/local/ec2/ec2-api-tools-1.7.5.1\" where the version number are specific to the version you downloaded. To get the right numbers, use : 1 ls /usr/local/ec2 We will also add the bin directory for the CLI tools to our system path. 1 export PATH=\" $ PATH : $ EC2_HOME /bin\" Set your identity for the CLI Tools Your access keys identify you to the Amazon EC2 CLI tools. There are two types of access keys: access key IDs (for example, AKIAIOSFODNN7EXAMPLE) and secret access keys (for example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY). You should have stored your access keys in a safe place when you received them. We will set the following environment variable which will serve as default values for the access and secret keys. This will save you from entering your keys for each command. 1 2 export AWS_ACCESS_KEY=your-aws-access-key-id export AWS_SECRET_KEY=your-aws-secret-key We can check that the keys have been properly set 1 ec2-describe-regions Which should yield something like : 1 2 3 4 5 6 7 8 9 >REGION eu-west-1 ec2.eu-west-1.amazonaws.com >REGION ap-southeast-1 ec2.ap-southeast-1.amazonaws.com >REGION ap-southeast-2 ec2.ap-southeast-2.amazonaws.com >REGION eu-central-1 ec2.eu-central-1.amazonaws.com >REGION ap-northeast-1 ec2.ap-northeast-1.amazonaws.com >REGION us-east-1 ec2.us-east-1.amazonaws.com >REGION sa-east-1 ec2.sa-east-1.amazonaws.com >REGION us-west-1 ec2.us-west-1.amazonaws.com >REGION us-west-2 ec2.us-west-2.amazonaws.com Change the region (if needed) The default EC2 CLI region is US East (us-east-1). To change this region, you need to set the following environment variable 1 export EC2_URL=https://<service_endpoint> where the service endpoint is something like ec2.region.amazonaws.com (cf ec2-describe-regions ). Connect via SSH In a terminal, go to the location of the private key file (.pem) used when launching the instance. In a command line shell, change directories to the location of the private key file that you created when you launched the instance. The SSH command should be something like : 1 ssh -i /path/my-key-pair.pem user_name@public_dns_name where use name is ubuntu for an ubuntu AMI and the public dns name is specified in the AWS console. Alternatively, the dns name can be found using 1 ec2-describe-instances For instance : 1 ssh -i /path/my-key-pair.pem ubuntu@ec2-52-62-114-212.ap-southeast-2.compute.amazonaws.com When prompted, enter yes. You should get something like : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Welcome to Ubuntu 14.04.2 LTS (GNU/Linux 3.13.0-48-generic x86_64) * Documentation: https://help.ubuntu.com/ System information as of Sun Jan 3 07:11:31 UTC 2016 System load: 0.0 Memory usage: 5% Processes: 82 Usage of /: 9.8% of 7.74GB Swap usage: 0% Users logged in: 0 Graph this data and manage this system at: https://landscape.canonical.com/ Get cloud support with Ubuntu Advantage Cloud Guest: http://www.ubuntu.com/business/services/cloud 0 packages can be updated. 0 updates are security updates. The programs included with the Ubuntu system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. Congrats, you are now connected ! You can transfer files easily with the scp command : 1 scp -i /path/my-key-pair.pem myfile ubuntu@ec2-52-62-114-212.ap-southeast-2.compute.amazonaws.com:~ 3. Close your an instance Select your instance in the AWS console. Actions , Instance State , Terminate . Choose Yes, Terminate .","tags":"AWS","title":"AWS Part 2 : EC2 instances"},{"url":"http://tdeboissiere.github.io/aws-part-1-setting-up-amazon-ec2.html","text":"1. Setup your account Start by signing up for an AWS account. Choose the free tier option : you won't be billed for 12 months given you do not exceed certain limits ! You will have to provide credit card information and a phone number. 2. Setup your user IAM What is IAM ? AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization). Amazon advises you against using your AWS account credentials to access AWS because the root account provides unrestricted access to your AWS resources. The preferred procedure is to for you to create and use an IAM user to whom you grant administrative permissions. Then you can access AWS with a special URL + IAM user credentials. To create a new IAM user : Create a group of administrators Open the IAM console Groups => Create New Group => Set Administrators as name (for instance) and click Next . In the policy list , tick the AdministratorAccess box => Next Step => Create Group . Voila ! Instructions as gif below: Create an IAM user for yourself Users => Create New Users => Set you username => Clear the box next to \"Generate an access key\" => Create . In the list of users , click on the newly created user. In the Groups tab, => Add User to Groups . Tick the checkbox near your group of administrators => Add to Groups . In the Security credentials tab => click Manage password => Assign a custom password . Confirm password => Apply . Voila ! To sign in as the IAM user : Sign out Go to the following URL : https://aws_id.signin.aws.amazon.com/console/ (your AWS ID is a 12 number ID which you can find in your account settings) Enter your IAM username and password You should now be signed in as : your_user_name @ your_aws_account_id . Instead of using your AWS ID, you can create an alias : IAM Dashboard => Customize => Enter your alias. You can now sign in with : - https://your_alias.signin.aws.amazon.com/console/ You should now see (in the IAM Dashboard ) that your IAM users sign-in link has indeed been changed to: - https://your_alias.signin.aws.amazon.com/console More information on IAM . 3. Create a Key Pair AWS uses public-key cryptography to secure the login information for your instance. A Linux instance has no password; you use a key pair to log in to your instance securely. You specify the name of the key pair when you launch your instance, then provide the private key when you log in using SSH. N.B. You need a key pair per region (if you launch instances in multiple regions). To create a key pair Sign in with your user IAM URL Select any region from the navigation bar => Key Pairs => Create Key Pair . Enter a name for your key pair (e.g. alias-key-pair-sydney) => Create . You automatically download the private key in a PEM file. Save it in a safe place N.B. This is the only chance to save the file. You will need to give your key pair name + corresponding private key each time you connect to the instance. If you connect by SSH to your Linux instance, in the directory where you put your private key: $ chmod 400 your_user_name-key-pair-region_name.pem to set the permissions of the private key file so that only you can read it. More information on Key Pair To connect to your instance using your key pair To connect to your Linux instance from a computer running Mac or Linux, you'll specify the .pem file to your SSH client with the -i option and the path to your private key. Open the Amazon EC2 console console 4. Create a security group Security groups act as a firewall for associated instances, controlling both inbound and outbound traffic at the instance level. Prerequisite You will need the public IP address of your computer. Type this command in a terminal to access your public IP address : curl -s http://checkip.amazonaws.com/ To create a security group with least privilege Go to the EC2 dashboard. Select Security Groups . Create Security Group . Give it a name and a description. Choose a VPC (the default one has a * symbol). On the Inbound tab, add the following access rules : HTTP, set Source to anywhere . HTTP, set Source to anywhere . SSH, set Source to your public IP in CIDR notation. Then click on Create . N.B. If your IP address is 203.0.113.25, then its CIDR notation is 203.0.113.25/32. What we've done here is allowing web servers to receive all inbound HTTP and HTTPS traffic as well as allowing SSH connection from your computer. More information on network security .","tags":"AWS","title":"AWS Part 1 : Setting up Amazon EC2"}]}